package edu.arizona.cs;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.*;

public class Evaluation {

    static String inputfilePathCSV = "src/main/resources/input.csv";

    public void Evaluation(String ResultFilePath_New, String ResultFilePath_Combined,
            String ResultFilePath_Original) {
        Set<String> groundTruthHate0 = readGroundTruthHate0(inputfilePathCSV, 0);
        Set<String> groundTruthHate1 = readGroundTruthHate0(inputfilePathCSV, 1);

        Map<String, Float> retrievedTweets_New = readRetrievedTweets(ResultFilePath_New);
        Map<String, Float> retrievedTweets_Combined = readRetrievedTweets(ResultFilePath_Combined);
        Map<String, Float> retrievedTweets_Original = readRetrievedTweets(ResultFilePath_Original);

        System.out.println(
                "Total number of Tweets NOT having hate speech in the original data: " + groundTruthHate0.size());
        System.out
                .println("Total number of Tweets HAVING hate speech in the original data: " + groundTruthHate1.size());
        System.out.println("Tweets retrieved using the query generated by Neural network using the new dataset : "
                + retrievedTweets_New.size());
        System.out.println("Tweets retrieved using the query generated by Neural network using the original dataset : "
                + retrievedTweets_Original.size());
        System.out.println(
                "Tweets retrieved using the query generated by Neural network using the new and original dataset, this is the improved version of the BERTweet Model : "
                        + retrievedTweets_Combined.size());

        System.out.println(
                "Evaluation of the model (Neural network + Traditional Approach) by using the queries generated by old dataset, new dataset and the improved model: ");

        System.out.println("Using the query generate by original dataset from Neural network: ");
        mearusingPRA(groundTruthHate0, groundTruthHate1, retrievedTweets_Original);

        System.out.println("Using the query generate by new dataset from Neural network: ");
        mearusingPRA(groundTruthHate0, groundTruthHate1, retrievedTweets_New);

        System.out.println("Using the query generate by improved Neural network model: ");
        mearusingPRA(groundTruthHate0, groundTruthHate1, retrievedTweets_Combined);
    }

    public static void mearusingPRA(Set<String> groundTruthHate0, Set<String> groundTruthHate1,
            Map<String, Float> retrievedTweets) {

        Map<String, Integer> hateValues0 = getHateValues(retrievedTweets, inputfilePathCSV, 0);
        Map<String, Integer> hateValues1 = getHateValues(retrievedTweets, inputfilePathCSV, 1);

        System.out.println("Number of normal tweets retrieved by the model (False Positive): " + hateValues0.size());
        System.out.println("Number of hate tweets retrieved by the model (True Positive): " + hateValues1.size());

        int totalRetrievedTweets = retrievedTweets.size();

        // Calculate TP, TF, FP, FN
        int truePositives = hateValues1.size();
        int falsePositives = hateValues0.size();
        int falseNegatives = groundTruthHate1.size() - hateValues1.size();
        int trueNegatives = groundTruthHate0.size() - hateValues0.size();
        System.out.println("Number of normal tweets NOT retrieved by the model (True Negative): " + trueNegatives);
        System.out.println("Number of hate tweets NOT retrieved by the model (False Negative): " + falseNegatives);

        // Calculate precision, recall, F1 score, and accuracy
        double precision = (double) truePositives / (truePositives + falsePositives);
        double recall = (double) truePositives / (truePositives + falseNegatives);
        double f1Score = 2 * (precision * recall) / (precision + recall);
        double accuracy = (double) (truePositives + trueNegatives) / (truePositives + trueNegatives + falsePositives
                + falseNegatives);

        // Print the metrics
        System.out.println("Precision: " + precision);
        System.out.println("Recall: " + recall);
        System.out.println("F1 Score: " + f1Score);
        System.out.println("Accuracy: " + accuracy);
    }

    public static Set<String> readGroundTruthHate0(String filePath, int value) {
        Set<String> groundTruthHate0 = new HashSet<>();
        try (BufferedReader br = new BufferedReader(new FileReader(filePath))) {
            String line;
            while ((line = br.readLine()) != null) {
                String[] values = line.split(",");
                if (values.length >= 2) {
                    String tweetId = values[0];
                    String hateValueStr = values[1];
                    try {
                        if (hateValueStr.equalsIgnoreCase("hate")) {
                            // Assuming that "hate" represents a hate value of 1
                            int hateValue = 1;
                            if (hateValue == value) {
                                groundTruthHate0.add(tweetId);
                            }
                        } else {
                            int hateValue = Integer.parseInt(hateValueStr);
                            if (hateValue == value) {
                                groundTruthHate0.add(tweetId);
                            }
                        }
                    } catch (NumberFormatException e) {
                        e.printStackTrace();
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return groundTruthHate0;
    }

    public static Map<String, Float> readRetrievedTweets(String filePath) {
        Map<String, Float> retrievedTweets = new HashMap<>();
        try (BufferedReader br = new BufferedReader(new FileReader(filePath))) {
            String line;
            while ((line = br.readLine()) != null) {
                String[] parts = line.split(":");
                Float tweetScore = Float.parseFloat(parts[1]);
                String tweet = parts[2].trim();
                retrievedTweets.put(tweet, tweetScore);
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
        return retrievedTweets;
    }

    public static Map<String, Integer> getHateValues(Map<String, Float> retrievedTweet, String filePath, int value) {
        Map<String, Integer> hateValuesHate0 = new HashMap<>();
        // Map<String, Integer> hateValuesHate1 = new HashMap<>();

        try (BufferedReader br = new BufferedReader(new FileReader(filePath))) {
            String line = br.readLine();
            int rowNumber = 2; // Start from the second row
            while ((line = br.readLine()) != null) {
                String[] values = line.split(",");
                String tweetId = Integer.toString(rowNumber); // Use row number as tweet ID
                rowNumber++;
                String tweet = values[0];
                String hateValueStr = values[1];
                if (retrievedTweet.containsKey(tweet)) {
                    if (hateValueStr.equalsIgnoreCase("hate")) {
                        // Assuming that "hate" represents a hate value of 1
                        int hateValue = 1;
                        if (hateValue == value) {
                            hateValuesHate0.put(tweet, hateValue);
                        }
                    } else {
                        int hateValue = Integer.parseInt(hateValueStr);
                        if (hateValue == value) {
                            hateValuesHate0.put(tweet, hateValue);
                        }
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

        return hateValuesHate0;
    }
}
